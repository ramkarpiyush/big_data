ğ‡ğ¨ğ° ğƒğğ¥ğ­ğš ğ‹ğšğ¤ğ ğ‡ğšğ§ğğ¥ğğ¬ ğ“ğ¢ğ¦ğ ğ“ğ«ğšğ¯ğğ¥ ğšğ§ğ ğ•ğğ«ğ¬ğ¢ğ¨ğ§ğ¢ğ§ğ  ğ¯ğ¢ğš _ğğğ¥ğ­ğš_ğ¥ğ¨ğ 

People describe Delta Lake's time travel as a way to access historical versions of data based on either the version number or the insertion timestamp â€” but how its internally doing?

â™»ï¸ ğŸ. ğ–ğ¡ğšğ­ ğ¢ğ¬ ğƒğğ¥ğ­ğš ğ‹ğšğ¤ğ ğ¬ğ¨ğ¥ğ¯ğ¢ğ§ğ ?
Data lakes (HDFS/S3) don't have ACID transactions
You can't UPDATE, DELETE, or ROLLBACK

Delta Lake brings:
-> ACID properties
-> Versioning
-> Time travel
-> Schema evolution

It achieves all this by introducing one core thing:
ğŸ‘‰ the _delta_log/ folder

â™»ï¸ ğŸ. ğ–ğ¡ğšğ­â€™ğ¬ ğ¢ğ§ğ¬ğ¢ğğ _ğğğ¥ğ­ğš_ğ¥ğ¨ğ /?
-> JSON files: transaction logs recording metadata, file changes, and schema updates.
Eg: 0000.json
-> Checkpoint files: Parquet files that consolidate many JSON logs into a single optimized snapshot for faster reads.
Eg: 0001000.checkpoint.parquet
-> CRC/Metadata files: Used for internal tracking

â™»ï¸ ğŸ‘. ğ‡ğ¨ğ° ğƒğğ¥ğ­ğš ğ“ğ«ğšğ§ğ¬ğšğœğ­ğ¢ğ¨ğ§ğ¬ ğ–ğ¨ğ«ğ¤
Every operation (INSERT/UPDATE/DELETE/MERGE) does this:
- Creates or modifies data files (Parquet)
- Logs changes in a new JSON file (_delta_log/), recording what changed
- The JSON contains: new files added, old files removed, schema, partition info, operation metrics
- Commits atomically
- Sequential JSONs = version history
- After N commits â†’ Delta writes a checkpoint for performance

â™»ï¸ ğŸ’. ğˆğ§ğ¬ğğ«ğ­ ğ„ğ±ğšğ¦ğ©ğ¥ğ
Insert Alice, Bob:

-> It creates a new Parquet file: part-00-abc.snappy.parquet
-> It writes the first log file:
_delta_log/0000.json
{
 "add": {
 "path": "part-00-abc.snappy.parquet",
 "size": 12345,
 "modificationTime": t1,
 "dataChange": true,
 "partitionValues": {}
 },
 "commitInfo": {
 "operation": "WRITE",
 "operationParameters": {
 "mode": "Append"
 },
 "isBlindAppend": true,
 "timestamp": t2
 }
}

â™»ï¸ ğŸ“. ğ„ğ±ğšğ¦ğ©ğ¥ğ: ğ”ğ©ğğšğ­ğ ğƒğšğ­ğš
Now you run an UPDATE changing Alice to "Alicia":

Delta writes:
-> A new Parquet file part-01-def.snappy.parquet (with updated record)
-> Marks old file part-00-abc.snappy.parquet as removed.
A new JSON log:
_delta_log/0000.json

{
 "add": {
 "path": "part-00001-def.snappy.parquet",
 "size": 56789,
 "modificationTime": t3,
 "dataChange": true,
 "partitionValues": {}
 },
 "remove": {
 "path": "part-00000-abc.snappy.parquet",
 "deletionTimestamp": t4,
 "dataChange": true
 },
 "commitInfo": {
 "operation": "UPDATE",
 "operationParameters": {},
 "timestamp": t5
 }
}

â™»ï¸ ğŸ”. ğ‡ğ¨ğ° ğ“ğ¢ğ¦ğ ğ“ğ«ğšğ¯ğğ¥ ğ–ğ¨ğ«ğ¤ğ¬
Delta knows:
Version 0 â†’ only part-00-abc
Version 1 â†’ part-01-def, part-00-abc is deleted

When you query:
SELECT * FROM customer VERSION AS OF 0
It reads commit 0000.json
It scans part-00-abc.snappy.parquet

When you query:
SELECT * FROM customer VERSION AS OF 1
It reads commit 0001.json
It scans part-01-def.snappy.parquet

â™»ï¸ Every write creates a new "snapshot" but does NOT physically delete immediately

#spark #data