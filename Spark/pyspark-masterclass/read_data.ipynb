{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470ddf23",
   "metadata": {},
   "source": [
    "# **Read Data in Pyspark:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6df016",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b8805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SparkSession creation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"Read-Data\").getOrCreate()\n",
    "\n",
    "# ## What happens internally?\n",
    "# Checks if a SparkSession already exists\n",
    "# If yes → returns it\n",
    "# If no → creates a new one\n",
    "# That’s why it’s called getOrCreate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34016487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-featured SparkSession (Production-style)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Read-Data-Application\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953e3d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|height|\n",
      "+---+------+------+\n",
      "|  1|Piyush| 177.5|\n",
      "|  2|   Bob| 175.0|\n",
      "+---+------+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Piyush\", 177.50), (2, \"Bob\", 175.00)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"height\"])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de270e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", \"true\")                # first row as header\n",
    "          .option(\"inferSchema\", \"true\")           # let Spark infer types (ok for small files)\n",
    "          .option(\"sep\", \",\")                      # delimiter\n",
    "          .option(\"quote\", '\"')                    # quote char\n",
    "          .option(\"escape\", '\"')                   # escape char\n",
    "          .option(\"multiLine\", \"true\")             # multi-line fields\n",
    "          .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "          .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "          .option(\"mode\", \"PERMISSIVE\")            # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "          .option(\"encoding\", \"UTF-8\")\n",
    "          .option(\"nullValue\", \"\")\n",
    "          .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "          .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "          .option(\"pathGlobFilter\", \"*.csv\")\n",
    "          .option(\"recursiveFileLookup\", \"true\")   # read nested folders\n",
    "          .load(r\"D:\\GitLocal\\Spark-The-Definitive-Guide\\data\\flight-data\\csv\\2015-summary.csv\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5380619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+---+---------------+--------+-------+--------------+------------+-----------------+-------------+\n",
      "|customer_id|invoice_no|gender|age|       category|quantity|  price|payment_method|invoice_date|    shopping_mall|_rescued_data|\n",
      "+-----------+----------+------+---+---------------+--------+-------+--------------+------------+-----------------+-------------+\n",
      "|        201|   I885979|Female| 26|       Clothing|       3| 900.24|    Debit Card|  2021-07-04|        Metrocity|         null|\n",
      "|        202|   I810217|Female| 51|       Clothing|       3| 900.24|          Cash|  2022-01-14|        Metrocity|         null|\n",
      "|        203|   I499170|Female| 38|           Toys|       1|  35.84|   Credit Card|  2022-02-20|           Kanyon|         null|\n",
      "|        204|   I792963|Female| 59|       Clothing|       5| 1500.4|    Debit Card|  2022-06-18|Emaar Square Mall|         null|\n",
      "|        205|   I311151|Female| 39|       Souvenir|       3|  35.19|   Credit Card|  2022-04-27| Mall of Istanbul|         null|\n",
      "|        206|   I219282|Female| 36|          Shoes|       4|2400.68|   Credit Card|  2021-04-12|     Istinye Park|         null|\n",
      "|        207|   I345856|Female| 51|          Shoes|       1| 600.17|   Credit Card|  2022-07-18|           Kanyon|         null|\n",
      "|        208|   I119121|Female| 61|Food & Beverage|       4|  20.92|          Cash|  2021-12-04| Mall of Istanbul|         null|\n",
      "|        209|   I223552|Female| 39|          Shoes|       3|1800.51|    Debit Card|  2022-09-24|      Cevahir AVM|         null|\n",
      "|        210|   I238961|  Male| 40|      Cosmetics|       5|  203.3|   Credit Card|  2021-06-08|   Forum Istanbul|         null|\n",
      "|        211|   I200890|  Male| 47|      Cosmetics|       5|  203.3|   Credit Card|  2023-01-29|           Kanyon|         null|\n",
      "|        212|   I331913|Female| 21|       Clothing|       2| 600.16|          Cash|  2021-11-30| Mall of Istanbul|         null|\n",
      "|        213|   I933623|Female| 43|          Shoes|       3|1800.51|   Credit Card|  2022-01-24| Mall of Istanbul|         null|\n",
      "|        214|   I169199|  Male| 52|          Shoes|       5|3000.85|          Cash|  2022-03-12|   Viaport Outlet|         null|\n",
      "|        215|   I214356|Female| 36|          Books|       4|   60.6|          Cash|  2021-12-03|     Metropol AVM|         null|\n",
      "|        216|   I293617|  Male| 23|     Technology|       3| 3150.0|          Cash|  2023-01-13|        Metrocity|         null|\n",
      "|        217|   I324223|Female| 39|           Toys|       4| 143.36|          Cash|  2022-05-07|        Metrocity|         null|\n",
      "|        218|   I623222|Female| 52|       Clothing|       3| 900.24|   Credit Card|  2023-03-07| Mall of Istanbul|         null|\n",
      "|        219|   I949680|Female| 41|       Souvenir|       1|  11.73|          Cash|  2022-09-16|   Viaport Outlet|         null|\n",
      "|        220|   I118780|Female| 55|       Clothing|       1| 300.08|          Cash|  2022-01-23|        Metrocity|         null|\n",
      "+-----------+----------+------+---+---------------+--------+-------+--------------+------------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = (spark.read\n",
    "              .format(\"parquet\")\n",
    "              .option(\"mergeSchema\", \"true\")         # merge evolved schemas\n",
    "              .option(\"pathGlobFilter\", \"*.parquet\")\n",
    "              .option(\"recursiveFileLookup\", \"true\")\n",
    "              .load(r\"D:\\GitLocal\\big_data\\Databricks\\databricks-masterclass\\data\\shoppinginvoices\\invoices_201_99457.parquet\"))\\\n",
    "              .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacb39e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = (spark.read\n",
    "           .format(\"json\")\n",
    "           .option(\"multiLine\", \"true\")           # needed for pretty/indented JSON\n",
    "           .option(\"primitivesAsString\", \"false\")\n",
    "           .option(\"allowUnquotedFieldNames\", \"false\")\n",
    "           .option(\"dropMalformed\", \"false\")      # use 'mode' instead for Spark 3.x\n",
    "           .option(\"mode\", \"PERMISSIVE\")          # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "           .option(\"samplingRatio\", \"1.0\")        # for inference\n",
    "           .option(\"pathGlobFilter\", \"*.json\")\n",
    "           .option(\"recursiveFileLookup\", \"true\")\n",
    "           .load(r\"D:\\GitLocal\\big_data\\Dataset\\flight-data2015.json\"))\\\n",
    "           .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd5993",
   "metadata": {},
   "source": [
    "## **File reading Modes:**\n",
    "1. PERMISSIVE (default): \n",
    "    - Loads all valid rows.\n",
    "    - For malformed rows, Spark puts null in columns and stores the bad record in a special column called _corrupt_record.\n",
    "    - For JSON, _corrupt_record is often auto-created\n",
    "    - For CSV, it is not, when schema is provided\n",
    "2. DROPMALFORMED → drops bad rows\n",
    "3. FAILFAST → aborts on first bad row\n",
    "\n",
    "### Supported File Formats for mode:\n",
    "1. CSV\n",
    "2. JSON\n",
    "3. Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47621e66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500e5ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+\n",
      "|    Name|Height|Age|\n",
      "+--------+------+---+\n",
      "|  Piyush| 177.5| 30|\n",
      "| Shivani|166.75| 26|\n",
      "|Akanksha| 150.0| 25|\n",
      "+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "Data = [\n",
    "    (\"Piyush\", 177.50, 30),\n",
    "    (\"Shivani\", 166.75, 26),\n",
    "    (\"Akanksha\", 150.00, 25)\n",
    "]\n",
    "Schema = StructType([\n",
    "    StructField(\"Name\", StringType(), False),\n",
    "    StructField(\"Height\", FloatType(), False),\n",
    "    StructField(\"Age\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data = Data, schema = Schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f960c5",
   "metadata": {},
   "source": [
    "### **Numeric Types**\n",
    "\n",
    "- **ByteType** → 8-bit integer (rarely inferred unless explicitly cast)\n",
    "- **ShortType** → 16-bit integer\n",
    "- **IntegerType** → 32-bit integer (common for whole numbers)\n",
    "- **LongType** → 64-bit integer (used for large integers)\n",
    "- **FloatType** → 32-bit floating point (less common; Spark usually picks Double)\n",
    "- **DoubleType** → 64-bit floating point (default for Python `float` and decimal numbers)\n",
    "- **DecimalType(precision, scale)** → For fixed-point decimals (inferred when reading from formats like Parquet/ORC with schema)\n",
    "\n",
    "### **String & Binary**\n",
    "- **StringType** → Text values\n",
    "- **BinaryType** → Raw binary data (e.g., images, files)\n",
    "\n",
    "### **Boolean**\n",
    "- **BooleanType** → `true` / `false`\n",
    "\n",
    "### **Date & Time**\n",
    "- **DateType** → `yyyy-MM-dd`\n",
    "- **TimestampType** → `yyyy-MM-dd HH:mm:ss` or ISO formats\n",
    "\n",
    "### **Complex Types**\n",
    "- **ArrayType(elementType)** → Lists or arrays\n",
    "- **MapType(keyType, valueType)** → Key-value pairs\n",
    "- **StructType(fields)** → Nested records (JSON objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d962775",
   "metadata": {},
   "source": [
    "\n",
    "# Reading CSV with Malformed records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7023a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+---------------------------------------------+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|_corrupt_record                              |\n",
      "+------------------------+-------------------+-----+---------------------------------------------+\n",
      "|United States           |Romania            |15   |null                                         |\n",
      "|United States           |Croatia            |1    |null                                         |\n",
      "|United States           |Ireland            |344  |null                                         |\n",
      "|Egypt                   |United States      |15   |null                                         |\n",
      "|United States           |India              |62   |null                                         |\n",
      "|United States           |Singapore          |1    |null                                         |\n",
      "|United States           |Grenada            |62   |null                                         |\n",
      "|Costa Rica              |United States      |588  |null                                         |\n",
      "|Senegal                 |United States      |null |Senegal,United States,40.10                  |\n",
      "|Moldova                 |United States      |1    |null                                         |\n",
      "|United States           |Sint Maarten       |325  |null                                         |\n",
      "|United States           |Marshall Islands   |39   |null                                         |\n",
      "|Guyana                  |United States      |64   |null                                         |\n",
      "|Malta                   |United States      |1    |null                                         |\n",
      "|Anguilla                |United States      |41   |null                                         |\n",
      "|Bolivia                 |United States      |30   |null                                         |\n",
      "|United States           |Paraguay           |6    |null                                         |\n",
      "|Algeria                 |United States      |4    |null                                         |\n",
      "|Turks and Caicos Islands|United States      |null |Turks and Caicos Islands,United States,230.20|\n",
      "|United States           |Gibraltar          |1    |null                                         |\n",
      "+------------------------+-------------------+-----+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"count\", IntegerType(), False),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_flight_csv = (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", \"true\")                # first row as header\n",
    "                    .schema(schema)           # let Spark infer types (ok for small files)\n",
    "                    .option(\"sep\", \",\")                      # delimiter\n",
    "                    .option(\"mode\", \"PERMISSIVE\")            # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "                    .option(\"encoding\", \"UTF-8\")\n",
    "                    .load(r\"D:\\GitLocal\\big_data\\Spark\\pyspark-masterclass\\flightdata-malformed.csv\"))\\\n",
    "                    \n",
    "df_flight_csv.show(truncate=False)      \n",
    "df_flight_csv.printSchema()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110bcdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+----+---------------------------------------------+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|age |_corrupt_record                              |\n",
      "+------------------------+-------------------+----+---------------------------------------------+\n",
      "|Senegal                 |United States      |null|Senegal,United States,40.10                  |\n",
      "|Turks and Caicos Islands|United States      |null|Turks and Caicos Islands,United States,230.20|\n",
      "|Luxembourg              |United States      |null|Luxembourg,United States,155.30              |\n",
      "|Hong Kong               |United States      |null|Hong Kong,United States,332.40               |\n",
      "|United States           |Guatemala          |null|United States,Guatemala,318.50               |\n",
      "+------------------------+-------------------+----+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_flight_csv.filter(col(\"_corrupt_record\").isNotNull()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daabd96",
   "metadata": {},
   "source": [
    "##### Filter `df_flight_csv` to retrieve malformed records captured in **PERMISSIVE** mode from the `_corrupt_record` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "901ea886",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"count\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4f69d",
   "metadata": {},
   "source": [
    "##### **FAILFAST:**\n",
    "- First malformed record → ❌ job fails immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc6e4dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o695.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 58.0 failed 1 times, most recent failure: Lost task 0.0 in stage 58.0 (TID 73) (impetus-bl0672.impetus.co.in executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [Senegal,United States,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 24 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n\tat java.lang.Integer.parseInt(Integer.java:580)\r\n\tat java.lang.Integer.parseInt(Integer.java:615)\r\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\r\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [Senegal,United States,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 24 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n\tat java.lang.Integer.parseInt(Integer.java:580)\r\n\tat java.lang.Integer.parseInt(Integer.java:615)\r\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m df_flight_csv_FAILFAST \u001b[38;5;241m=\u001b[39m (spark\u001b[38;5;241m.\u001b[39mread\n\u001b[0;32m      2\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)                \u001b[38;5;66;03m# first row as header\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m                     \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitLocal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbig_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpyspark-masterclass\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mflightdata-malformed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\\\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdf_flight_csv_FAILFAST\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m      \n\u001b[0;32m     11\u001b[0m df_flight_csv_FAILFAST\u001b[38;5;241m.\u001b[39mprintSchema() \n",
      "File \u001b[1;32mc:\\Users\\piyush.ramkar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\piyush.ramkar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\piyush.ramkar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\piyush.ramkar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\piyush.ramkar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o695.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 58.0 failed 1 times, most recent failure: Lost task 0.0 in stage 58.0 (TID 73) (impetus-bl0672.impetus.co.in executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [Senegal,United States,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 24 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n\tat java.lang.Integer.parseInt(Integer.java:580)\r\n\tat java.lang.Integer.parseInt(Integer.java:615)\r\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\r\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [Senegal,United States,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 24 more\r\nCaused by: java.lang.NumberFormatException: For input string: \"40.10\"\r\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n\tat java.lang.Integer.parseInt(Integer.java:580)\r\n\tat java.lang.Integer.parseInt(Integer.java:615)\r\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:292)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:190)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:347)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "df_flight_csv_FAILFAST = (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", \"true\")                # first row as header\n",
    "                    .schema(schema2)           # let Spark infer types (ok for small files)\n",
    "                    .option(\"sep\", \",\")                      # delimiter\n",
    "                    .option(\"mode\", \"FAILFAST\")            # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "                    .option(\"encoding\", \"UTF-8\")\n",
    "                    .load(r\"D:\\GitLocal\\big_data\\Spark\\pyspark-masterclass\\flightdata-malformed.csv\"))\\\n",
    "                    \n",
    "df_flight_csv_FAILFAST.show(truncate=False)      \n",
    "df_flight_csv_FAILFAST.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90a88d",
   "metadata": {},
   "source": [
    "##### **DROPMALFORMED:**\n",
    "- Drop the row that contains marformed records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0aa875a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------------------+-------------------+-----+\n",
      "|United States                   |Romania            |15   |\n",
      "|United States                   |Croatia            |1    |\n",
      "|United States                   |Ireland            |344  |\n",
      "|Egypt                           |United States      |15   |\n",
      "|United States                   |India              |62   |\n",
      "|United States                   |Singapore          |1    |\n",
      "|United States                   |Grenada            |62   |\n",
      "|Costa Rica                      |United States      |588  |\n",
      "|Moldova                         |United States      |1    |\n",
      "|United States                   |Sint Maarten       |325  |\n",
      "|United States                   |Marshall Islands   |39   |\n",
      "|Guyana                          |United States      |64   |\n",
      "|Malta                           |United States      |1    |\n",
      "|Anguilla                        |United States      |41   |\n",
      "|Bolivia                         |United States      |30   |\n",
      "|United States                   |Paraguay           |6    |\n",
      "|Algeria                         |United States      |4    |\n",
      "|United States                   |Gibraltar          |1    |\n",
      "|Saint Vincent and the Grenadines|United States      |1    |\n",
      "|Italy                           |United States      |382  |\n",
      "+--------------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flight_csv_DROPMALFORMED = (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", \"true\")               # first row as header\n",
    "                    .schema(schema2)                         # let Spark infer types (ok for small files)\n",
    "                    .option(\"sep\", \",\")                     # delimiter\n",
    "                    .option(\"mode\", \"DROPMALFORMED\")             # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "                    .option(\"encoding\", \"UTF-8\")\n",
    "                    .load(r\"D:\\GitLocal\\big_data\\Spark\\pyspark-masterclass\\flightdata-malformed.csv\"))\\\n",
    "                    \n",
    "df_flight_csv_DROPMALFORMED.show(20, truncate=False)      \n",
    "df_flight_csv_DROPMALFORMED.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3fae7",
   "metadata": {},
   "source": [
    "### **Note:** *When reading a CSV file in PySpark with an explicit schema, `_corrupt_record` does NOT appear automatically, even in PERMISSIVE mode. It must be explicitly defined in the schema.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb599f",
   "metadata": {},
   "source": [
    "# Reading JSON with Malformed records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4b20359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----+----------------------------------------------------------------------------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|count|_corrupt_record                                                                   |\n",
      "+-------------------+-----------------+-----+----------------------------------------------------------------------------------+\n",
      "|Romania            |United States    |15   |null                                                                              |\n",
      "|Croatia            |United States    |1    |null                                                                              |\n",
      "|Ireland            |United States    |344  |null                                                                              |\n",
      "|United States      |Egypt            |15   |null                                                                              |\n",
      "|India              |United States    |62   |null                                                                              |\n",
      "|Singapore          |United States    |1    |null                                                                              |\n",
      "|Grenada            |United States    |62   |null                                                                              |\n",
      "|United States      |Costa Rica       |588  |null                                                                              |\n",
      "|United States      |Senegal          |40   |null                                                                              |\n",
      "|United States      |Moldova          |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Moldova\",\"count\":1.00}|\n",
      "+-------------------+-----------------+-----+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-----+------------------------------------------------------------------------------------------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME  |count|_corrupt_record                                                                                 |\n",
      "+-------------------+-------------------+-----+------------------------------------------------------------------------------------------------+\n",
      "|United States      |Moldova            |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Moldova\",\"count\":1.00}              |\n",
      "|Gibraltar          |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Gibraltar\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1.00}            |\n",
      "|United States      |Honduras           |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Honduras\",\"count\":362.00}           |\n",
      "|United States      |Trinidad and Tobago|null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Trinidad and Tobago\",\"count\":211.00}|\n",
      "|Suriname           |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Suriname\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":34.00}            |\n",
      "|Palau              |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Palau\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":31.00}               |\n",
      "|United States      |New Zealand        |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"New Zealand\",\"count\":111.00}        |\n",
      "|Egypt              |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Egypt\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":12.00}               |\n",
      "|Turkey             |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Turkey\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":129.00}             |\n",
      "|United States      |Malaysia           |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Malaysia\",\"count\":\"Two\"}            |\n",
      "|Georgia            |United States      |null |{\"ORIGIN_COUNTRY_NAME\":\"Georgia\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1.00}              |\n",
      "|United States      |Cuba               |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Cuba\",\"count\":\"Four Six Six\"}       |\n",
      "|United States      |Cook Islands       |null |{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Cook Islands\",\"count\":13.00}        |\n",
      "+-------------------+-------------------+-----+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), False),\n",
    "    StructField(\"count\", IntegerType(), False)\n",
    "    ,StructField(\"_corrupt_record\", StringType(), True)\n",
    "\n",
    "])\n",
    "\n",
    "df_json = (spark.read\n",
    "           .format(\"json\")\n",
    "           .schema(schema)\n",
    "        #    .option(\"multiLine\", \"true\")           # needed for pretty/indented JSON\n",
    "        #    .option(\"dropMalformed\", \"false\")      # use 'mode' instead for Spark 3.x\n",
    "           .option(\"mode\", \"PERMISSIVE\")            ## PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "           .load(r\"D:\\GitLocal\\big_data\\Spark\\pyspark-masterclass\\flightdata-malformed.json\"))\n",
    "\n",
    "df_json.show(10, truncate=False)\n",
    "df_json.printSchema()\n",
    "df_json.filter(col(\"_corrupt_record\").isNotNull()).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
