{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b7d2ce",
   "metadata": {},
   "source": [
    "# 1. Polling or Retrying for Job Completion or File Availability:\n",
    "- You are waiting for a file to arrive in a specific folder (like /data/input/) before starting your processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f154d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "while not os.path.exists(\"/data/input/daily_file.csv\"):\n",
    "    print(\"Waiting for file...\")\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In data engineering, `while` loops are not as commonly used as higher-level constructs like DataFrame transformations (e.g., `map`, `filter`, `groupBy`) in Spark or SQL-style queries in databases. However, `while` loops can be very useful in certain scenarios, especially in orchestration, control flow, and low-level scripting.\n",
    "\n",
    "Here are some **common use cases of `while` loops in data engineering**:\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ 1. **Polling or Retrying for Job Completion or File Availability**\n",
    "\n",
    "Used to check periodically if a file has landed in a folder (like in S3, HDFS) or if an external process (e.g., an ETL job or API call) has completed.\n",
    "\n",
    "```python\n",
    "import time\n",
    "import os\n",
    "\n",
    "while not os.path.exists(\"/data/input/daily_file.csv\"):\n",
    "    print(\"Waiting for file...\")\n",
    "    time.sleep(60)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ 2. **Retry Logic for Unstable Connections**\n",
    "\n",
    "Especially for APIs or flaky DB connections.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://api.example.com/data\"\n",
    "max_retries = 5\n",
    "attempts = 0\n",
    "\n",
    "while attempts < max_retries:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Retrying due to error: {e}\")\n",
    "    attempts += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üì§ 3. **Paginated API Calls**\n",
    "\n",
    "Some APIs return data in pages; you need to loop until all pages are fetched.\n",
    "\n",
    "```python\n",
    "page = 1\n",
    "while True:\n",
    "    data = fetch_data_from_api(page=page)\n",
    "    if not data:\n",
    "        break\n",
    "    process_data(data)\n",
    "    page += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä 4. **Streaming Data Processing Loops**\n",
    "\n",
    "Useful in Kafka consumers, Spark Structured Streaming, or custom streaming solutions.\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    message = kafka_consumer.poll()\n",
    "    if message:\n",
    "        process_message(message)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üßπ 5. **Batch Processing Large Datasets**\n",
    "\n",
    "Breaking a huge task into chunks (e.g., reading 1 million records at a time).\n",
    "\n",
    "```python\n",
    "offset = 0\n",
    "batch_size = 100000\n",
    "\n",
    "while True:\n",
    "    df = read_data(offset, batch_size)\n",
    "    if df.empty:\n",
    "        break\n",
    "    process(df)\n",
    "    offset += batch_size\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è 6. **Partition-wise Processing**\n",
    "\n",
    "In file systems like HDFS/S3 or partitioned databases.\n",
    "\n",
    "```python\n",
    "for partition in partitions:\n",
    "    offset = 0\n",
    "    while True:\n",
    "        batch = read_partition_data(partition, offset, batch_size)\n",
    "        if not batch:\n",
    "            break\n",
    "        process(batch)\n",
    "        offset += batch_size\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è 7. **Loop Until a Condition in a Workflow Is Met**\n",
    "\n",
    "E.g., in Airflow or custom workflows: keep checking if a downstream system is ready.\n",
    "\n",
    "```python\n",
    "ready = check_downstream_system_ready()\n",
    "while not ready:\n",
    "    print(\"Waiting for downstream system...\")\n",
    "    time.sleep(30)\n",
    "    ready = check_downstream_system_ready()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like examples specific to **PySpark**, **Airflow**, or **cloud-based pipelines** (e.g., AWS Glue, Azure Data Factory)?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
